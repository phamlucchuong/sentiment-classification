{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8593afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ import t·∫•t c·∫£ th∆∞ vi·ªán\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE  # ƒê·ªÉ x·ª≠ l√Ω imbalanced data\n",
    "import joblib\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "# Th√™m th∆∞ m·ª•c cha c·ªßa notebooks v√†o sys.path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import v√† reload ƒë·ªÉ l·∫•y phi√™n b·∫£n m·ªõi nh·∫•t\n",
    "from utils import embedding\n",
    "importlib.reload(embedding)\n",
    "from utils.embedding import get_phobert_embeddings_batch\n",
    "\n",
    "print(\"‚úì ƒê√£ import t·∫•t c·∫£ th∆∞ vi·ªán\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acf4f314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è  S·ª≠ d·ª•ng device: cuda\n",
      "   GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "   Memory: 6.05 GB\n",
      "‚úì ƒê√£ load PhoBERT model\n"
     ]
    }
   ],
   "source": [
    "# Load d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω\n",
    "df = pd.read_csv('../data/processed/cleaned_reviews.csv')\n",
    "\n",
    "# Load PhoBERT v√† chuy·ªÉn sang GPU n·∫øu c√≥\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è  S·ª≠ d·ª•ng device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "phobert.to(device)\n",
    "print(\"‚úì ƒê√£ load PhoBERT model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1aec49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù T·ªïng s·ªë m·∫´u h·ª£p l·ªá: 22822 (ƒë√£ lo·∫°i b·ªè 21 m·∫´u kh√¥ng h·ª£p l·ªá)\n",
      "üîÑ ƒêang t·∫°o embeddings tr√™n cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T·∫°o embeddings:   0%|          | 0/357 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T·∫°o embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 357/357 [03:38<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ t·∫°o embeddings v·ªõi shape: (22822, 768)\n",
      "‚úì L∆∞u embeddings v√†o '../data/processed/embeddings.npy'\n",
      "‚úì L∆∞u labels v√†o '../data/processed/labels.npy'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# T·∫°o embeddings v·ªõi batch processing (t·∫≠n d·ª•ng GPU hi·ªáu qu·∫£ h∆°n)\n",
    "# Lo·∫°i b·ªè c√°c d√≤ng c√≥ review_cleaned r·ªóng ho·∫∑c NaN\n",
    "df_clean = df.dropna(subset=['review_cleaned'])\n",
    "df_clean = df_clean[df_clean['review_cleaned'].str.strip() != ''].reset_index(drop=True)\n",
    "\n",
    "print(f\"üìù T·ªïng s·ªë m·∫´u h·ª£p l·ªá: {len(df_clean)} (ƒë√£ lo·∫°i b·ªè {len(df) - len(df_clean)} m·∫´u kh√¥ng h·ª£p l·ªá)\")\n",
    "print(f\"üîÑ ƒêang t·∫°o embeddings tr√™n {device}...\")\n",
    "\n",
    "# S·ª≠ d·ª•ng batch processing ƒë·ªÉ t·∫≠n d·ª•ng GPU\n",
    "batch_size = 64 if torch.cuda.is_available() else 32\n",
    "embeddings = get_phobert_embeddings_batch(\n",
    "    df_clean['review_cleaned'].tolist(), \n",
    "    phobert, \n",
    "    tokenizer, \n",
    "    device,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "np.save('../data/processed/embeddings.npy', embeddings)\n",
    "np.save('../data/processed/labels.npy', df_clean['label'].values)\n",
    "print(f\"‚úì ƒê√£ t·∫°o embeddings v·ªõi shape: {embeddings.shape}\")\n",
    "print(f\"‚úì L∆∞u embeddings v√†o '../data/processed/embeddings.npy'\")\n",
    "print(f\"‚úì L∆∞u labels v√†o '../data/processed/labels.npy'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91e259ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Chia d·ªØ li·ªáu:\n",
      "- Train: 13692 m·∫´u\n",
      "- Validation: 4565 m·∫´u\n",
      "- Test: 4565 m·∫´u\n",
      "\n",
      "‚ö†Ô∏è  Ph√¢n b·ªë nh√£n trong train:\n",
      "- Negative: 3291 (24.0%)\n",
      "- Positive: 10401 (76.0%)\n",
      "\n",
      "ü§ñ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán c√°c m√¥ h√¨nh (v·ªõi class_weight='balanced')...\n",
      "\n",
      "‚è≥ ƒêang hu·∫•n luy·ªán Logistic Regression...\n",
      "‚úÖ Logistic Regression:\n",
      "   - Train: 89.29%\n",
      "   - Val:   88.54%\n",
      "   - Test:  88.50%\n",
      "   - Overfitting gap: 0.75%\n",
      "\n",
      "‚è≥ ƒêang hu·∫•n luy·ªán Random Forest...\n",
      "‚úÖ Random Forest:\n",
      "   - Train: 97.63%\n",
      "   - Val:   91.48%\n",
      "   - Test:  91.37%\n",
      "   - Overfitting gap: 6.15%\n",
      "\n",
      "‚è≥ ƒêang hu·∫•n luy·ªán SVM...\n",
      "‚úÖ SVM:\n",
      "   - Train: 91.27%\n",
      "   - Val:   90.21%\n",
      "   - Test:  89.86%\n",
      "   - Overfitting gap: 1.06%\n",
      "\n",
      "============================================================\n",
      "üìà K·∫æT QU·∫¢ HU·∫§N LUY·ªÜN T·∫§T C·∫¢ C√ÅC M√î H√åNH (s·∫Øp x·∫øp theo Val):\n",
      "============================================================\n",
      "Random Forest       : Val=91.48% | Test=91.37%\n",
      "SVM                 : Val=90.21% | Test=89.86%\n",
      "Logistic Regression : Val=88.54% | Test=88.50%\n"
     ]
    }
   ],
   "source": [
    "# Chia train/validation/test (60/20/20) ƒë·ªÉ theo d√µi overfitting\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    embeddings, df_clean['label'].values, test_size=0.2, random_state=42, stratify=df_clean['label']\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp  # 0.25 * 0.8 = 0.2\n",
    ")\n",
    "\n",
    "print(f\"üìä Chia d·ªØ li·ªáu:\")\n",
    "print(f\"- Train: {len(X_train)} m·∫´u\")\n",
    "print(f\"- Validation: {len(X_val)} m·∫´u\")\n",
    "print(f\"- Test: {len(X_test)} m·∫´u\")\n",
    "print(f\"\\n‚ö†Ô∏è  Ph√¢n b·ªë nh√£n trong train:\")\n",
    "print(f\"- Negative: {(y_train == 0).sum()} ({100*(y_train == 0).sum()/len(y_train):.1f}%)\")\n",
    "print(f\"- Positive: {(y_train == 1).sum()} ({100*(y_train == 1).sum()/len(y_train):.1f}%)\")\n",
    "\n",
    "# Hu·∫•n luy·ªán c√°c model v·ªõi class balancing v√† regularization m·∫°nh\n",
    "print(\"\\nü§ñ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán c√°c m√¥ h√¨nh (v·ªõi class_weight='balanced')...\\n\")\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000, \n",
    "        C=0.1,  # Regularization m·∫°nh h∆°n (gi·∫£m t·ª´ 1.0 xu·ªëng 0.1)\n",
    "        class_weight='balanced',  # X·ª≠ l√Ω imbalanced data\n",
    "        random_state=42\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=50,  # Gi·∫£m t·ª´ 100 xu·ªëng 50\n",
    "        max_depth=10,  # Gi·ªõi h·∫°n ƒë·ªô s√¢u ƒë·ªÉ gi·∫£m overfitting\n",
    "        min_samples_split=20,  # TƒÉng t·ª´ 2 l√™n 20\n",
    "        min_samples_leaf=10,  # TƒÉng t·ª´ 1 l√™n 10\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    ),\n",
    "    'SVM': SVC(\n",
    "        kernel='rbf',\n",
    "        C=0.5,  # Regularization m·∫°nh h∆°n (gi·∫£m t·ª´ 1.0 xu·ªëng 0.5)\n",
    "        gamma='scale',\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "results = {}\n",
    "results_val = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"‚è≥ ƒêang hu·∫•n luy·ªán {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # ƒê√°nh gi√° tr√™n c·∫£ train, validation v√† test\n",
    "    train_acc = accuracy_score(y_train, model.predict(X_train))\n",
    "    val_acc = accuracy_score(y_val, model.predict(X_val))\n",
    "    test_acc = accuracy_score(y_test, model.predict(X_test))\n",
    "    \n",
    "    results[name] = test_acc\n",
    "    results_val[name] = val_acc\n",
    "    \n",
    "    print(f\"‚úÖ {name}:\")\n",
    "    print(f\"   - Train: {100 * train_acc:.2f}%\")\n",
    "    print(f\"   - Val:   {100 * val_acc:.2f}%\")\n",
    "    print(f\"   - Test:  {100 * test_acc:.2f}%\")\n",
    "    print(f\"   - Overfitting gap: {100 * (train_acc - val_acc):.2f}%\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìà K·∫æT QU·∫¢ HU·∫§N LUY·ªÜN T·∫§T C·∫¢ C√ÅC M√î H√åNH (s·∫Øp x·∫øp theo Val):\")\n",
    "print(\"=\" * 60)\n",
    "for name in sorted(results_val, key=results_val.get, reverse=True):\n",
    "    print(f\"{name:20s}: Val={100*results_val[name]:.2f}% | Test={100*results[name]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b94f0869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ √Åp d·ª•ng SMOTE ƒë·ªÉ c√¢n b·∫±ng d·ªØ li·ªáu training...\n",
      "Tr∆∞·ªõc SMOTE:\n",
      "  - Negative: 3291\n",
      "  - Positive: 10401\n",
      "  - T·ª∑ l·ªá: 3.16:1\n",
      "\n",
      "Sau SMOTE:\n",
      "  - Negative: 10401\n",
      "  - Positive: 10401\n",
      "  - T·ª∑ l·ªá: 1.00:1\n",
      "\n",
      "‚úÖ ƒê√£ t·∫°o th√™m 7110 m·∫´u synthetic\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ‚ö° C·∫¢I TI·∫æN: √Åp d·ª•ng SMOTE ƒë·ªÉ c√¢n b·∫±ng d·ªØ li·ªáu training\n",
    "# ============================================================\n",
    "print(\"\\nüîÑ √Åp d·ª•ng SMOTE ƒë·ªÉ c√¢n b·∫±ng d·ªØ li·ªáu training...\")\n",
    "print(f\"Tr∆∞·ªõc SMOTE:\")\n",
    "print(f\"  - Negative: {(y_train == 0).sum()}\")\n",
    "print(f\"  - Positive: {(y_train == 1).sum()}\")\n",
    "print(f\"  - T·ª∑ l·ªá: {(y_train == 1).sum() / (y_train == 0).sum():.2f}:1\")\n",
    "\n",
    "# √Åp d·ª•ng SMOTE\n",
    "smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"\\nSau SMOTE:\")\n",
    "print(f\"  - Negative: {(y_train_balanced == 0).sum()}\")\n",
    "print(f\"  - Positive: {(y_train_balanced == 1).sum()}\")\n",
    "print(f\"  - T·ª∑ l·ªá: {(y_train_balanced == 1).sum() / (y_train_balanced == 0).sum():.2f}:1\")\n",
    "print(f\"\\n‚úÖ ƒê√£ t·∫°o th√™m {len(X_train_balanced) - len(X_train)} m·∫´u synthetic\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31938523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Hu·∫•n luy·ªán l·∫°i v·ªõi d·ªØ li·ªáu ƒë√£ c√¢n b·∫±ng b·ªüi SMOTE...\n",
      "\n",
      "‚è≥ ƒêang hu·∫•n luy·ªán Logistic Regression (SMOTE)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucchuong/Documents/sentiment-classification/training/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Hu·∫•n luy·ªán l·∫°i v·ªõi d·ªØ li·ªáu ƒë√£ c√¢n b·∫±ng b·ªüi SMOTE...\n",
      "\n",
      "‚è≥ ƒêang hu·∫•n luy·ªán Logistic Regression (SMOTE)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucchuong/Documents/sentiment-classification/training/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logistic Regression (SMOTE):\n",
      "   - Train: 88.68%\n",
      "   - Val:   88.52%\n",
      "   - Test:  88.37%\n",
      "   - Overfitting gap: 0.16%\n",
      "   - D·ª± ƒëo√°n Val: Neg=1329 (29.1%), Pos=3236 (70.9%)\n",
      "\n",
      "‚è≥ ƒêang hu·∫•n luy·ªán Random Forest (SMOTE)...\n",
      "‚úÖ Random Forest (SMOTE):\n",
      "   - Train: 94.03%\n",
      "   - Val:   88.54%\n",
      "   - Test:  88.17%\n",
      "   - Overfitting gap: 5.49%\n",
      "   - D·ª± ƒëo√°n Val: Neg=1244 (27.3%), Pos=3321 (72.7%)\n",
      "\n",
      "‚è≥ ƒêang hu·∫•n luy·ªán SVM (SMOTE)...\n",
      "‚úÖ SVM (SMOTE):\n",
      "   - Train: 90.14%\n",
      "   - Val:   89.57%\n",
      "   - Test:  89.24%\n",
      "   - Overfitting gap: 0.57%\n",
      "   - D·ª± ƒëo√°n Val: Neg=1193 (26.1%), Pos=3372 (73.9%)\n",
      "\n",
      "======================================================================\n",
      "üìä SO S√ÅNH K·∫æT QU·∫¢: CLASS_WEIGHT vs SMOTE\n",
      "======================================================================\n",
      "\n",
      "üîµ V·ªõi class_weight='balanced':\n",
      "  Random Forest            : Val=91.48% | Test=91.37%\n",
      "  SVM                      : Val=90.21% | Test=89.86%\n",
      "  Logistic Regression      : Val=88.54% | Test=88.50%\n",
      "\n",
      "üü¢ V·ªõi SMOTE:\n",
      "  SVM (SMOTE)              : Val=89.57% | Test=89.24%\n",
      "  Random Forest (SMOTE)    : Val=88.54% | Test=88.17%\n",
      "  Logistic Regression (SMOTE): Val=88.52% | Test=88.37%\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ü§ñ TRAIN L·∫†I V·ªöI D·ªÆ LI·ªÜU ƒê√É C√ÇN B·∫∞NG\n",
    "# ============================================================\n",
    "print(\"üöÄ Hu·∫•n luy·ªán l·∫°i v·ªõi d·ªØ li·ªáu ƒë√£ c√¢n b·∫±ng b·ªüi SMOTE...\\n\")\n",
    "\n",
    "# ƒê·ªãnh nghƒ©a models v·ªõi regularization C·ª∞C M·∫†NH\n",
    "models_balanced = {\n",
    "    'Logistic Regression (SMOTE)': LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        C=0.01,  # Regularization C·ª∞C M·∫†NH (gi·∫£m t·ª´ 0.1 xu·ªëng 0.01)\n",
    "        penalty='l2',\n",
    "        solver='lbfgs',\n",
    "        random_state=42,\n",
    "        # Kh√¥ng d√πng class_weight n·ªØa v√¨ ƒë√£ SMOTE\n",
    "    ),\n",
    "    'Random Forest (SMOTE)': RandomForestClassifier(\n",
    "        n_estimators=30,  # Gi·∫£m xu·ªëng 30\n",
    "        max_depth=8,  # Gi·∫£m xu·ªëng 8\n",
    "        min_samples_split=30,  # TƒÉng l√™n 30\n",
    "        min_samples_leaf=15,  # TƒÉng l√™n 15\n",
    "        max_features='sqrt',  # Gi·ªõi h·∫°n features\n",
    "        random_state=42,\n",
    "    ),\n",
    "    'SVM (SMOTE)': SVC(\n",
    "        kernel='rbf',\n",
    "        C=0.1,  # Regularization m·∫°nh h∆°n\n",
    "        gamma='scale',\n",
    "        random_state=42,\n",
    "        probability=True  # ƒê·ªÉ c√≥ th·ªÉ d√πng predict_proba\n",
    "    )\n",
    "}\n",
    "\n",
    "results_balanced = {}\n",
    "results_val_balanced = {}\n",
    "\n",
    "for name, model in models_balanced.items():\n",
    "    print(f\"‚è≥ ƒêang hu·∫•n luy·ªán {name}...\")\n",
    "    \n",
    "    # Train tr√™n d·ªØ li·ªáu ƒë√£ balanced\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "    \n",
    "    # ƒê√°nh gi√° tr√™n t·∫•t c·∫£ c√°c t·∫≠p\n",
    "    train_acc = accuracy_score(y_train_balanced, model.predict(X_train_balanced))\n",
    "    val_acc = accuracy_score(y_val, model.predict(X_val))\n",
    "    test_acc = accuracy_score(y_test, model.predict(X_test))\n",
    "    \n",
    "    # D·ª± ƒëo√°n tr√™n validation ƒë·ªÉ ki·ªÉm tra ph√¢n b·ªë\n",
    "    y_val_pred_temp = model.predict(X_val)\n",
    "    neg_pred = (y_val_pred_temp == 0).sum()\n",
    "    pos_pred = (y_val_pred_temp == 1).sum()\n",
    "    \n",
    "    results_balanced[name] = test_acc\n",
    "    results_val_balanced[name] = val_acc\n",
    "    \n",
    "    print(f\"‚úÖ {name}:\")\n",
    "    print(f\"   - Train: {100 * train_acc:.2f}%\")\n",
    "    print(f\"   - Val:   {100 * val_acc:.2f}%\")\n",
    "    print(f\"   - Test:  {100 * test_acc:.2f}%\")\n",
    "    print(f\"   - Overfitting gap: {100 * (train_acc - val_acc):.2f}%\")\n",
    "    print(f\"   - D·ª± ƒëo√°n Val: Neg={neg_pred} ({100*neg_pred/len(y_val):.1f}%), Pos={pos_pred} ({100*pos_pred/len(y_val):.1f}%)\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üìä SO S√ÅNH K·∫æT QU·∫¢: CLASS_WEIGHT vs SMOTE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüîµ V·ªõi class_weight='balanced':\")\n",
    "for name in sorted(results_val, key=results_val.get, reverse=True):\n",
    "    print(f\"  {name:25s}: Val={100*results_val[name]:.2f}% | Test={100*results[name]:.2f}%\")\n",
    "\n",
    "print(\"\\nüü¢ V·ªõi SMOTE:\")\n",
    "for name in sorted(results_val_balanced, key=results_val_balanced.get, reverse=True):\n",
    "    print(f\"  {name:25s}: Val={100*results_val_balanced[name]:.2f}% | Test={100*results_balanced[name]:.2f}%\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d30dbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üèÜ MODEL T·ªêT NH·∫§T (SMOTE): SVM (SMOTE)\n",
      "======================================================================\n",
      "üéØ Validation Accuracy: 89.57%\n",
      "üéØ Test Accuracy: 89.24%\n",
      "‚úì ƒê√£ l∆∞u model v√†o '../models/best_sentiment_model.pkl'\n",
      "\n",
      "üìä PH√ÇN T√çCH CHI TI·∫æT TR√äN VALIDATION SET:\n",
      "\n",
      "Ph√¢n b·ªë D·ª∞ ƒêO√ÅN: Counter({np.int64(1): 3372, np.int64(0): 1193})\n",
      "Ph√¢n b·ªë TH·ª∞C T·∫æ: Counter({np.int64(1): 3468, np.int64(0): 1097})\n",
      "\n",
      "- Negative:\n",
      "  D·ª± ƒëo√°n: 1193 (26.1%)\n",
      "  Th·ª±c t·∫ø:  1097 (24.0%)\n",
      "\n",
      "- Positive:\n",
      "  D·ª± ƒëo√°n: 3372 (73.9%)\n",
      "  Th·ª±c t·∫ø:  3468 (76.0%)\n",
      "\n",
      "üìã CLASSIFICATION REPORT (Validation Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative      0.760     0.827     0.792      1097\n",
      "    Positive      0.944     0.918     0.930      3468\n",
      "\n",
      "    accuracy                          0.896      4565\n",
      "   macro avg      0.852     0.872     0.861      4565\n",
      "weighted avg      0.900     0.896     0.897      4565\n",
      "\n",
      "\n",
      "üìä CONFUSION MATRIX (Validation Set):\n",
      "                Predicted\n",
      "              Neg    Pos\n",
      "Actual Neg    907    190\n",
      "       Pos    286   3182\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üèÜ CH·ªåN V√Ä L∆ØU MODEL T·ªêT NH·∫§T T·ª™ SMOTE\n",
    "# ============================================================\n",
    "# Ch·ªçn model t·ªët nh·∫•t d·ª±a tr√™n validation accuracy t·ª´ SMOTE\n",
    "best_model_name_new = max(results_val_balanced, key=results_val_balanced.get)\n",
    "best_model_new = models_balanced[best_model_name_new]\n",
    "\n",
    "# L∆∞u model m·ªõi\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "joblib.dump(best_model_new, '../models/best_sentiment_model.pkl')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üèÜ MODEL T·ªêT NH·∫§T (SMOTE):\", best_model_name_new)\n",
    "print(\"=\" * 70)\n",
    "print(f\"üéØ Validation Accuracy: {100 * results_val_balanced[best_model_name_new]:.2f}%\")\n",
    "print(f\"üéØ Test Accuracy: {100 * results_balanced[best_model_name_new]:.2f}%\")\n",
    "print(f\"‚úì ƒê√£ l∆∞u model v√†o '../models/best_sentiment_model.pkl'\")\n",
    "\n",
    "# Ph√¢n t√≠ch chi ti·∫øt tr√™n validation set\n",
    "print(\"\\nüìä PH√ÇN T√çCH CHI TI·∫æT TR√äN VALIDATION SET:\")\n",
    "y_val_pred_final = best_model_new.predict(X_val)\n",
    "\n",
    "from collections import Counter\n",
    "print(f\"\\nPh√¢n b·ªë D·ª∞ ƒêO√ÅN: {Counter(y_val_pred_final)}\")\n",
    "print(f\"Ph√¢n b·ªë TH·ª∞C T·∫æ: {Counter(y_val)}\")\n",
    "\n",
    "print(f\"\\n- Negative:\")\n",
    "print(f\"  D·ª± ƒëo√°n: {(y_val_pred_final == 0).sum()} ({100*(y_val_pred_final == 0).sum()/len(y_val):.1f}%)\")\n",
    "print(f\"  Th·ª±c t·∫ø:  {(y_val == 0).sum()} ({100*(y_val == 0).sum()/len(y_val):.1f}%)\")\n",
    "\n",
    "print(f\"\\n- Positive:\")\n",
    "print(f\"  D·ª± ƒëo√°n: {(y_val_pred_final == 1).sum()} ({100*(y_val_pred_final == 1).sum()/len(y_val):.1f}%)\")\n",
    "print(f\"  Th·ª±c t·∫ø:  {(y_val == 1).sum()} ({100*(y_val == 1).sum()/len(y_val):.1f}%)\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nüìã CLASSIFICATION REPORT (Validation Set):\")\n",
    "print(classification_report(y_val, y_val_pred_final, \n",
    "                          target_names=['Negative', 'Positive'],\n",
    "                          digits=3))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nüìä CONFUSION MATRIX (Validation Set):\")\n",
    "cm = confusion_matrix(y_val, y_val_pred_final)\n",
    "print(f\"                Predicted\")\n",
    "print(f\"              Neg    Pos\")\n",
    "print(f\"Actual Neg   {cm[0][0]:4d}   {cm[0][1]:4d}\")\n",
    "print(f\"       Pos   {cm[1][0]:4d}   {cm[1][1]:4d}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dff2c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ M√î H√åNH T·ªêT NH·∫§T: Random Forest\n",
      "üéØ Validation Accuracy: 91.48%\n",
      "üéØ Test Accuracy: 91.37%\n",
      "‚úì ƒê√£ l∆∞u m√¥ h√¨nh v√†o '../models/best_sentiment_model.pkl'\n",
      "\n",
      "üìä Ph√¢n t√≠ch d·ª± ƒëo√°n tr√™n Validation Set:\n",
      "Ph√¢n b·ªë d·ª± ƒëo√°n: Counter({np.int64(1): 3573, np.int64(0): 992})\n",
      "Ph√¢n b·ªë th·ª±c t·∫ø: Counter({np.int64(1): 3468, np.int64(0): 1097})\n",
      "- Negative (0): 992 d·ª± ƒëo√°n vs 1097 th·ª±c t·∫ø\n",
      "- Positive (1): 3573 d·ª± ƒëo√°n vs 3468 th·ª±c t·∫ø\n"
     ]
    }
   ],
   "source": [
    "# Ch·ªçn m√¥ h√¨nh t·ªët nh·∫•t d·ª±a tr√™n validation accuracy\n",
    "best_model_name = max(results_val, key=results_val.get)\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c models n·∫øu ch∆∞a c√≥\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "joblib.dump(best_model, '../models/best_sentiment_model.pkl')\n",
    "print(f\"\\nüèÜ M√î H√åNH T·ªêT NH·∫§T: {best_model_name}\")\n",
    "print(f\"üéØ Validation Accuracy: {100 * results_val[best_model_name]:.2f}%\")\n",
    "print(f\"üéØ Test Accuracy: {100 * results[best_model_name]:.2f}%\")\n",
    "print(f\"‚úì ƒê√£ l∆∞u m√¥ h√¨nh v√†o '../models/best_sentiment_model.pkl'\")\n",
    "\n",
    "# Ki·ªÉm tra d·ª± ƒëo√°n tr√™n validation set\n",
    "print(\"\\nüìä Ph√¢n t√≠ch d·ª± ƒëo√°n tr√™n Validation Set:\")\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "from collections import Counter\n",
    "print(f\"Ph√¢n b·ªë d·ª± ƒëo√°n: {Counter(y_val_pred)}\")\n",
    "print(f\"Ph√¢n b·ªë th·ª±c t·∫ø: {Counter(y_val)}\")\n",
    "print(f\"- Negative (0): {(y_val_pred == 0).sum()} d·ª± ƒëo√°n vs {(y_val == 0).sum()} th·ª±c t·∫ø\")\n",
    "print(f\"- Positive (1): {(y_val_pred == 1).sum()} d·ª± ƒëo√°n vs {(y_val == 1).sum()} th·ª±c t·∫ø\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
